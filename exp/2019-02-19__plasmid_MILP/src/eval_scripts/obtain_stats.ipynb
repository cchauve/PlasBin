{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plasmids MILP experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "The following describes the results of the experiment with the first version of the Plasmids assembly MILP. There are two parts to the experiment. In the first part, I tried to determine the set of $\\alpha$s in the objective function by running the MILP for the same sample. In the second part, I used the same $\\alpha$ values for different samples. In both parts, I have recorded the average and maximum precision, recall and F1 score obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "plt.switch_backend('agg')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '../../output'\n",
    "ratio_tests = os.path.join(output_dir,'ratio_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(line, stat_dict, file, folder_loc):\n",
    "    stat = line.split(\" \")[-1]\n",
    "    stat_dict[file.split('/')[folder_loc]].append(float(stat))\n",
    "    return stat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean(mean, k, precs, recs, f1s):\n",
    "    mean[k] = {}\n",
    "    mean[k]['precision'] = sum(precs[k])/len(precs[k])\n",
    "    mean[k]['recall'] = sum(recs[k])/len(recs[k])\n",
    "    mean[k]['f1_score'] = sum(f1s[k])/len(f1s[k])\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_max(best, k, precs, recs, f1s):\n",
    "    best[k] = {}\n",
    "    best[k]['precision'] = max(precs[k])\n",
    "    best[k]['recall'] = max(recs[k])\n",
    "    best[k]['f1_score'] = max(f1s[k])\n",
    "    return best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Statistics for various coefficient combinations ($\\alpha_1, \\alpha_2, \\alpha_3$)\n",
    "\n",
    "The aim of this part is to find which set of $\\alpha$ values would be a suitable choice for the MILP. For this part I used a single sample (id=103) and obtained the results for various $\\alpha$ values. The sample itself contains exactly 1 plasmid. However, I ran the experiments for number of plasmids (nplasmids) $\\in \\{1,2,3,4\\}$. The reason behind this is that in many cases, the output when nplasmids $= 1$ yielded 0 precision and recall. In other words, the best plasmid according to the MILP did not match the reference plasmid for the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-75423fc8d544>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mprecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"recall\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mrecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"f1\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mf1s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-fe8b030f7ecd>\u001b[0m in \u001b[0;36mupdate_dict\u001b[0;34m(line, stat_dict, file, folder_loc)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupdate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mstat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mstat_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfolder_loc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstat_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "files = [os.path.join(dp, f) for dp, dn, filenames in os.walk(ratio_tests) for f in filenames if \"eval.csv\" in f]\n",
    "precs, recs, f1s = defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            if \"precision\" in line:\n",
    "                precs = update_dict(line, precs, file, 11)\n",
    "            if \"recall\" in line:\n",
    "                recs = update_dict(line, recs, file, 11)\n",
    "            if \"f1\" in line:\n",
    "                f1s = update_dict(line, f1s, file, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, best = {}, {} \n",
    "mean_scores = []\n",
    "best_scores = []\n",
    "\n",
    "for ratio in precs:\n",
    "    mean = compute_mean(mean, ratio, precs, recs, f1s)\n",
    "    mean_scores.append([ratio, mean[ratio]['precision'], mean[ratio]['recall'], mean[ratio]['f1_score']])\n",
    "    best = compute_max(best, ratio, precs, recs, f1s)\n",
    "    best_scores.append([ratio, best[ratio]['precision'], best[ratio]['recall'], best[ratio]['f1_score']])\n",
    "    \n",
    "mean_scores = pd.DataFrame(mean_scores)\n",
    "mean_scores.rename(columns = {0: 'Ratio', 1: 'Precision', 2: 'Recall', 3: 'F1 score'}, inplace = True)\n",
    "\n",
    "best_scores = pd.DataFrame(best_scores)\n",
    "best_scores.rename(columns = {0: 'Ratio', 1: 'Precision', 2: 'Recall', 3: 'F1 score'}, inplace = True)\n",
    "\n",
    "with open(os.path.join(output_dir,'exp1_scores.csv'), 'w') as f:\n",
    "    mean_scores.to_csv(f, sep = '\\t', encoding='utf-8', index=False)\n",
    "with open(os.path.join(output_dir,'exp1_scores.csv'), 'a') as f:\n",
    "    best_scores.to_csv(f, sep = '\\t', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 1 lists the results. The first column lists the ratio ($\\alpha_1: \\alpha_2: \\alpha_3$) used in the objective function. From the table, we can see that the choice of $\\alpha_1$ does not impact the results too much. For instance, the cases 0.1.0, 1.1.0 and 2.1.0 have very similar precision, recall and F1 score. This indicates that the MILP assigns read depths rd[p][c] such that the deviation from the average read depth mean_rd[p] is negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Statistics for different samples\n",
    "\n",
    "In this part, I chose the $\\alpha$ ratios which performed better than others. If two sets of ratios gave the same or similar output, (such as 1.1.0 and 2.1.0 above), only one of the two was chosen. Since variation of $\\alpha_1$ does not significantly impact the answers, it is assigned the value 1. The ratios chosen for this part are 1.1.0, 1.1.1, 1.2.1 and 1.5.1. For each ratio, the MILP was run with nplasmids $\\in \\{1,2,3\\}$. In many cases, the MILP did not converge to the optimal for higher number of plasmids. \n",
    "\n",
    "A set of 10 ids was chosen to test the general performance of the MILP. These were the same ids on which Robert had previously carried out his preliminary experiments with the HyAsP greedy algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [os.path.join(dp, f) for dp, dn, filenames in os.walk(output_dir) for f in filenames if \"eval.csv\" in f]\n",
    "precs, recs, f1s = defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "\n",
    "for file in files:\n",
    "    if 'sample' in file.split('/')[10]:\n",
    "        sample_id = file.split('/')[10].split('_')[1]\n",
    "        with open(file, 'r') as f:\n",
    "            for line in f:\n",
    "                if \"precision\" in line:\n",
    "                    precs = update_dict(line, precs, file, 10)\n",
    "                if \"recall\" in line:\n",
    "                    recs = update_dict(line, recs, file, 10)\n",
    "                if \"f1\" in line:\n",
    "                    f1s = update_dict(line, f1s, file, 10)\n",
    "\n",
    "#print(precs)                    \n",
    "#print(len(precs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, best = {}, {}\n",
    "mean_scores, best_scores = [], [] \n",
    "for sample_id in precs:\n",
    "    #print(sample_id)\n",
    "    #print(precs[sample_id])\n",
    "    #print(sum(precs[sample_id]))\n",
    "    #print(len(precs[sample_id]))\n",
    "    mean[sample_id] = {}\n",
    "    mean[sample_id]['precision'] = sum(precs[sample_id])/len(precs[sample_id])\n",
    "    mean[sample_id]['recall'] = sum(recs[sample_id])/len(recs[sample_id])\n",
    "    mean[sample_id]['f1_score'] = sum(f1s[sample_id])/len(f1s[sample_id])\n",
    "    #mean = compute_mean(mean, sample_id, precs, recs, f1s)\n",
    "    mean_scores.append([sample_id, mean[sample_id]['precision'], mean[sample_id]['recall'], mean[sample_id]['f1_score']])\n",
    "\n",
    "    best[sample_id] = {}\n",
    "    best[sample_id]['precision'] = max(precs[sample_id])\n",
    "    best[sample_id]['recall'] = max(recs[sample_id])\n",
    "    best[sample_id]['f1_score'] = max(f1s[sample_id])   \n",
    "    #best = compute_max(best, sample_id, precs, recs, f1s)\n",
    "    best_scores.append([sample_id, best[sample_id]['precision'], best[sample_id]['recall'], best[sample_id]['f1_score']])\n",
    "\n",
    "mean_scores = pd.DataFrame(mean_scores)\n",
    "mean_scores.rename(columns = {0: 'Sample', 1: 'Precision', 2: 'Recall', 3: 'F1 score'}, inplace = True)\n",
    "\n",
    "best_scores = pd.DataFrame(best_scores)\n",
    "best_scores.rename(columns = {0: 'Sample', 1: 'Precision', 2: 'Recall', 3: 'F1 score'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "ind = np.arange(N)\n",
    "width = 0.27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the following figure, the average recall rate is lower than 0.6 for 6 of the 10\n",
    "samples chosen. However, the precision is higher than the recall for most of the\n",
    "samples. One explanation for this is that the MILP predicts a large plasmid\n",
    "than covers a significant portion of the reference plasmid. As one of the terms\n",
    "in the objective function increases the gene density, the MILP tends to choose\n",
    "a higher length plasmid. Hopefully, this issue might be resolved in the next\n",
    "experiment, in which path constraints will be introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "pvals = mean_scores['Precision'].values.tolist()\n",
    "rects1 = ax.bar(ind, pvals, width, color='r')\n",
    "rvals = mean_scores['Recall'].values.tolist()\n",
    "rects2 = ax.bar(ind+width, rvals, width, color='g')\n",
    "fvals = mean_scores['F1 score'].values.tolist()\n",
    "rects3 = ax.bar(ind+width*2, fvals, width, color='b')\n",
    "ids = mean_scores['Sample'].values.tolist()\n",
    "ids = [k.split('_')[1] for k in ids]\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_xticks(ind+width)\n",
    "ax.set_xticklabels( (ids[0], ids[1], ids[2], ids[3], ids[4], ids[5], ids[6], ids[7], ids[8], ids[9]) )\n",
    "ax.legend( (rects1[0], rects2[0], rects3[0]), ('Prec', 'Rec', 'F1') )\n",
    "plt.show()\n",
    "plt.savefig(os.path.join(output_dir,'mean_scores_MILP_exp1.pdf'), format = 'pdf', dpi = 1200, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "pvals = best_scores['Precision'].values.tolist()\n",
    "rects1 = ax.bar(ind, pvals, width, color='r')\n",
    "rvals = best_scores['Recall'].values.tolist()\n",
    "rects2 = ax.bar(ind+width, rvals, width, color='g')\n",
    "fvals = best_scores['F1 score'].values.tolist()\n",
    "rects3 = ax.bar(ind+width*2, fvals, width, color='b')\n",
    "ids = best_scores['Sample'].values.tolist()\n",
    "ids = [k.split('_')[1] for k in ids]\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_xticks(ind+width)\n",
    "ax.set_xticklabels( (ids[0], ids[1], ids[2], ids[3], ids[4], ids[5], ids[6], ids[7], ids[8], ids[9]) )\n",
    "ax.legend( (rects1[0], rects2[0], rects3[0]), ('Prec', 'Rec', 'F1') )\n",
    "plt.show()\n",
    "plt.savefig(os.path.join(output_dir,'best_scores_MILP_exp1.pdf'), format = 'pdf', dpi = 1200, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
